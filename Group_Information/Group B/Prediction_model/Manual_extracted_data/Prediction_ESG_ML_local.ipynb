{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loading storage data\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"data2.csv\")\n",
    "    \n",
    "    # Renaming columns\n",
    "    column_mapping = {col: col.strip().lower().replace('&', 'and') for col in df.columns}\n",
    "    return df.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the company name and number(year) from the column name \"Company\"\n",
    "def split_company_name_and_year(df):\n",
    "    df[['company', 'year']] = df['company'].str.extract(r'^(.*?)-(\\d{4})$')\n",
    "#extract year is after \"-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced preprocessing\n",
    "def advanced_preprocessing(df):\n",
    "    df = df[df['esg_score'].notna()].copy()\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "    \n",
    "    # Feature engineering of different year\n",
    "    df['years_active'] = df.groupby('company')['year'].transform(lambda x: x - x.min() + 1)\n",
    "    df['year_diff'] = df.groupby('company')['year'].transform(lambda x: x.diff().fillna(1))\n",
    "    \n",
    "    # Auto fill missing values\n",
    "    num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    for col in num_cols:\n",
    "        df[col] = df.groupby('company')[col].transform(\n",
    "            lambda x: x.fillna(x.mean() if x.mean() > 0 else 0)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preapre data\n",
    "df = load_data()\n",
    "split_company_name_and_year(df)\n",
    "processed_df = advanced_preprocessing(df)\n",
    "\n",
    "# features engineering\n",
    "X = processed_df.drop(columns=['esg_score', 'company', 'year'])\n",
    "y = processed_df['esg_score']\n",
    "groups = processed_df['company']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "models = {\n",
    "    'RandomForest': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=4,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=42\n",
    "        )\n",
    "    ),\n",
    "    'SVR': make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='rbf', C=1.0, epsilon=0.1)),\n",
    "\n",
    "    'GradientBoosting': make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )),\n",
    "     'XGBoost': make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )),\n",
    "\n",
    "\n",
    "    'AdaBoost': make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        StandardScaler(),\n",
    "        AdaBoostRegressor(\n",
    "            estimator=DecisionTreeRegressor(max_depth=3),\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.5,\n",
    "            random_state=42\n",
    "        )\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "    'MLP': make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "),\n",
    "\n",
    "'XGBoost1': make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=2,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "),\n",
    "\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation：\n",
      "RandomForest    | MAE: 5.51 ± 0.88\n",
      "SVR             | MAE: 8.09 ± 2.08\n",
      "GradientBoosting | MAE: 4.76 ± 0.65\n",
      "XGBoost         | MAE: 4.08 ± 0.84\n",
      "AdaBoost        | MAE: 4.54 ± 1.26\n",
      "MLP             | MAE: 2988015.89 ± 4216450.21\n",
      "XGBoost1        | MAE: 3.87 ± 1.09\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "print(\"Model evaluation：\")\n",
    "for name, model in models.items():\n",
    "    mae_scores = []\n",
    "    for train_idx, test_idx in gkf.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        mae_scores.append(mean_absolute_error(y_test, preds))\n",
    "    \n",
    "    print(f\"{name:15} | MAE: {np.mean(mae_scores):.2f} ± {np.std(mae_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Top 5：\n",
      "certification            0.960217\n",
      "current_gender           0.017804\n",
      "emission_intensities     0.007470\n",
      "total_waste_generated    0.007294\n",
      "absolute_emission        0.003113\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# analysis of feature importance\n",
    "print(\"\\nXGBoost Top 5：\")\n",
    "rf_model = models['XGBoost1'].steps[2][1]\n",
    "importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "print(importances.sort_values(ascending=False).head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model training \n",
    "final_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=2,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    ").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction model\n",
    "def predict_esg(model, current_data):\n",
    "    # generate time feather (not keep company/year）\n",
    "    current_data = current_data.copy()\n",
    "    current_data['years_active'] = current_data['year'] - current_data['year'].min() + 1\n",
    "    current_data['year_diff'] = 1\n",
    "    \n",
    "    required_features = X.columns.tolist()  \n",
    "    missing_features = set(required_features) - set(current_data.columns)\n",
    "    \n",
    "    # Fill in missing features\n",
    "    for feat in missing_features:\n",
    "        current_data[feat] = 0\n",
    "    \n",
    "    return model.predict(current_data[required_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    column_mapping = {col: col.strip().lower().replace('&', 'and') for col in df.columns}\n",
    "    return df.rename(columns=column_mapping)\n",
    "\n",
    "\n",
    "# Process latest data \n",
    "def load_latest_row():\n",
    "    # read csv\n",
    "    df = pd.read_csv(\"consolidated_esg_single_row.csv\")\n",
    "    df = standardize_columns(df)\n",
    "    latest_row = df.iloc[-1].to_dict()\n",
    "    \n",
    "    # trun in DataFrame\n",
    "    latest_df = pd.DataFrame([latest_row])\n",
    "    \n",
    "    # add year column if not exist\n",
    "    if 'year' not in latest_df.columns:\n",
    "        latest_df['year'] = 2024  # 添加默认年份\n",
    "    \n",
    "    return latest_df\n",
    "\n",
    "# load ESG score\n",
    "def load_esg_scores():\n",
    "    esg_df = pd.read_csv(\"esg_scored_result.csv\")\n",
    "    last_row = esg_df.iloc[-1]\n",
    "    esg_score_1 = last_row.iloc[-2]  \n",
    "    esg_score_2 = last_row.iloc[-1]  \n",
    "    return esg_score_1, esg_score_2\n",
    "\n",
    "# process new data \n",
    "latest_data = load_latest_row()\n",
    "\n",
    "# get ESG scores\n",
    "esg_score_1, esg_score_2 = load_esg_scores()\n",
    "\n",
    "#add nessasary columns\n",
    "latest_data['esg_score'] = esg_score_1\n",
    "latest_data['grade'] = esg_score_2\n",
    "\n",
    "processed_latest = advanced_preprocessing(latest_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict value ：44.00\n"
     ]
    }
   ],
   "source": [
    "pred_score = predict_esg(final_model, processed_latest)\n",
    "print(f\"Predict value ：{pred_score[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted grade: C\n"
     ]
    }
   ],
   "source": [
    "def convert_grade(score):\n",
    "    if pd.isna(score):\n",
    "        return None\n",
    "    if score < 30:\n",
    "        return 'D'\n",
    "    elif score < 40:\n",
    "        return 'C-'\n",
    "    elif score < 50:\n",
    "        return 'C'\n",
    "    elif score < 60:\n",
    "        return 'C+'\n",
    "    elif score < 67:\n",
    "        return 'B-'\n",
    "    elif score < 73:\n",
    "        return 'B'\n",
    "    elif score < 78:\n",
    "        return 'B+'\n",
    "    elif score < 83:\n",
    "        return 'A-'\n",
    "    elif score < 88:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'A+'\n",
    "    \n",
    "\n",
    "    # Convert the predicted score to a letter grade\n",
    "predicted_grade = convert_grade(pred_score[0])\n",
    "print(f\"Predicted grade: {predicted_grade}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
